`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/home/dora/miniconda3/envs/dora/bin/dora-distil-whisper", line 7, in <module>
    sys.exit(main())
  File "/home/dora/dora_ws/dora_robot_single/python_nodes/dora-distil-whisper/dora_distil_whisper/main.py", line 219, in main
    pipe = load_model()
  File "/home/dora/dora_ws/dora_robot_single/python_nodes/dora-distil-whisper/dora_distil_whisper/main.py", line 137, in load_model
    model.to(device)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4459, in to
    return super().to(*args, **kwargs)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
  File "/home/dora/miniconda3/envs/dora/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.43 GiB of which 5.19 MiB is free. Process 114035 has 76.04 MiB memory in use. Process 124940 has 15.69 GiB memory in use. Process 124935 has 2.25 GiB memory in use. Process 129247 has 3.84 GiB memory in use. Including non-PyTorch memory, this process has 1.50 GiB memory in use. Of the allocated memory 1.15 GiB is allocated by PyTorch, and 107.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[INFO] 使用本地缓存模型: /home/dora/.cache/huggingface/hub/models--openai--whisper-large-v3-turbo/snapshots/41f01f3fe87f28c78e2fbf8b568835947dd65ed9
